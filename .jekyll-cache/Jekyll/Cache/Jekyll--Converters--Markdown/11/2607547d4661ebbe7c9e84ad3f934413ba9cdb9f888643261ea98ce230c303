I"\,<h3 id="inlg-2018-generation-of-company-descriptions-using-concept-to-text-and-text-to-text-deep-models-dataset-collection-and-systems-evaluation">[INLG 2018] <a href="https://www.aclweb.org/anthology/W18-6532">Generation of Company descriptions using concept-to-text and text-to-text deep models: dataset collection and systems evaluation</a></h3>

<ul>
  <li>study the performance of several state-of-the-art sequence-tosequence models applied to generation of short company descriptions</li>
  <li>a newly created and publicly available company dataset that has been collected from Wikipedia, which consists of around <strong>51K company descriptions</strong> <a href="https://gricad-gitlab.univ-grenoble-alpes.fr/getalp/wikipediacompanycorpus">(Download)</a> that can be used for both concept-to-text and text-to-text generation tasks</li>
  <li>the dataset is <strong>not ideal for machine learning</strong> since the abstract, the body and the infobox are only <strong>loosely correlated</strong></li>
</ul>

<hr />
<p><img align="center" src="http://localhost:5000/images/tgfif/wiki.png" alt="drawing" width="400" /></p>

<hr />
<ul>
  <li>the basic model used for generating company description is based on the <a href="https://github.com/google/seq2seq/"><strong>RNN seq2seq model architecture</strong> (Sutskever et al., 2014)</a> which is divided into two main blocks: <strong>encoder</strong> which encodes the input sentence into fixed-length vector, and the <strong>decoder</strong> that decodes the vector into sequence of words</li>
  <li>as pointed out by (See et al., 2017) the classical seq2seq models suffer from two commonly known problems: repetition of subsequences and wording off-topic</li>
  <li>repetition is caused at the decoding stage, when the <strong>decoder relies too much on the previous output leading to infinite cycle</strong>; to deal with this problem is to use a coverage mechanism (Tu et al., 2016): used in machine translation, uses the <strong>attention weights to penalize the decoder</strong> for attending to input that has already been attended to previously</li>
  <li>hallucination can appear when the <strong>word to predict is infrequent</strong> in the training set and therefore has a poor word embedding making it close to a lot of other words; <a href="https://github.com/abisee/pointer-generator"><strong>Pointer-Generator Network (See et al., 2017)</strong> </a> which computes a generation probability p_gen ∈ [0, 1]. This value evaluates the probability of ‘generating’ a word based on the vocabulary known by the model, versus copying a word from the source</li>
  <li>standard automatic measures BLEU (Papineni et al., 2002), ROUGE-L (Lin and Hovy, 2003), Meteor (Denkowski and Lavie, 2014) and CIDEr (Vedantam et al., 2015) were computed using the E2E challenge script.</li>
</ul>

<h3 id="acl2019-multi-style-generative-reading-comprehension">[ACL2019] <a href="https://arxiv.org/pdf/1901.02262.pdf">Multi-Style Generative Reading Comprehension</a></h3>

<ul>
  <li>reading comprehension (RC) is a challenge task to answer a question given textual evidence provided in a document set</li>
  <li>generative models suffer from <strong>a dearth of training data</strong> to cover open-domain questions</li>
  <li>tackles <strong>generative reading comprehension</strong> (RC), which consists of <strong>answering questions</strong> based on textual evidence and <strong>natural language generation</strong> (NLG)
    <ul>
      <li>focuses on generating a summary from the question and multiple passages instead of extracting an answer span from the provided passages –&gt;  introduce the <a href="https://arxiv.org/pdf/1704.04368.pdf">pointer-generator mechanism</a> (See et al., 2017) for generating an abstractive answer from the question and multiple passages by extending to a <strong>Transformer</strong> one that allows words to be <strong>generated from a vocabulary</strong> and to be <strong>copied from the question and passages</strong></li>
      <li>learns <strong>multi-style answers</strong> within a model to improve the NLG capability for all styles involved –&gt; also extend the pointer-generator to <strong>a conditional decoder</strong> by introducing an artificial token corresponding to each style</li>
    </ul>
  </li>
</ul>

<p><img align="center" src="http://localhost:5000/images/tgfif/masque.png" alt="masque" width="450" />
<img align="center" src="http://localhost:5000/images/tgfif/multisource.png" alt="multisource" width="400" /></p>

<h3 id="naacl2019-keyphrase-generation-a-text-summarization-struggle">[NAACL2019] <a href="https://www.aclweb.org/anthology/N19-1070">Keyphrase Generation: A Text Summarization Struggle</a></h3>

<ul>
  <li>Problems of most existing keyphrase generation methods:
    <ul>
      <li>First, they are not able to find <strong>an optimal value for N</strong> (number of keywords to generate for an article) based on article contents and require it as a preset parameter.</li>
      <li>Second, the <strong>semantic and syntactic properties</strong> of article phrases (considered as candidate keywords) are <strong>analyzed separately</strong>. The meaning of longer text units like paragraphs or entire abstract/paper is missed.</li>
      <li>Third, <strong>only phrases that do appear in the paper are returned</strong>. In practice, authors do often assign words that are not part of their article.</li>
    </ul>
  </li>
  <li>Meng et al. (2017) overcome the second and third problem using an <strong>encoder-decoder model (COPYRNN)</strong> with a bidirectional Gated Recurrent Unit (GRU) and a forward GRU with attention.</li>
  <li>we explore <strong>abstractive text summarization models</strong> proposed in the literature, trained with article abstracts and titles as sources and keyword strings as targets.</li>
  <li><a href="https://arxiv.org/pdf/1704.04368.pdf"><strong>Pointer-Generator network (POINTCOV)</strong></a> is composed of an <strong>attention-based encoder</strong> that produces the context vector. The decoder is extended with a <strong>pointer-generator model</strong> that computes a probability p_gen from the context vector, the decoder states, and the decoder output.</li>
</ul>

<p><img align="center" src="http://localhost:5000/images/tgfif/scores.png" alt="scores" width="650" /></p>

<ul>
  <li>The results show that the tried text summarization models perform poorly on full-match keyword predictions. Their higher ROUGE scores further indicate that the problem is not entirely in the summarization process.</li>
</ul>

<h3 id="acl-2017-deep-keyphrase-generation-code">[ACL 2017] <a href="https://aclweb.org/anthology/P17-1054">Deep Keyphrase Generation</a> <a href="https://github.com/memray/seq2seq-keyphrase">[code]</a></h3>

<ul>
  <li>propose a generative model for keyphrase prediction with an encoder-decoder framework
    <ul>
      <li>could identify keyphrases that do not appear in the text</li>
      <li>capture the real semantic meaning behind the text</li>
    </ul>
  </li>
  <li>To apply the RNN Encoder-Decoder model, the data need to be converted into <strong>text-keyphrase pairs</strong> that contain only <strong>one source sequence and one target sequence</strong></li>
  <li>the RNN is <strong>not able to recall</strong> any keyphrase that contains out-ofvocabulary words –&gt; <a href="https://www.aclweb.org/anthology/P16-1154">copying mechanism (Gu et al., 2016)</a> is one feasible solution that enables RNN to predict <strong>out-of-vocabulary words</strong> by <strong>selecting appropriate words</strong> from the source text
<img src="http://localhost:5000/images/tgfif/copyrnn.png" alt="copyrnn model" /></li>
</ul>

<hr />

<h3 id="acl-2017-get-to-the-point-summarization-with-pointer-generator-networks-tensorflow-python2-tensorflow-python3-pytorch-python2-data-prepossessing">[ACL 2017] <a href="https://arxiv.org/pdf/1704.04368.pdf">Get To The Point: Summarization with Pointer-Generator Networks</a> <a href="https://github.com/abisee/pointer-generator">[Tensorflow Python2]</a> <a href="https://github.com/becxer/pointer-generator/">[Tensorflow Python3]</a> <a href="https://github.com/atulkum/pointer_summarizer">[Pytorch Python2]</a> <a href="https://github.com/abisee/cnn-dailymail">[Data Prepossessing]</a></h3>

<ul>
  <li>Neural <strong>sequence-to-sequence models</strong> have provided a viable new approach for abstractive text summarization but have two shortcomings:
    <ul>
      <li>they are liable to reproduce <strong>factual details inaccurately</strong></li>
      <li>they tend to <strong>repeat</strong> themselves</li>
    </ul>
  </li>
  <li>augments the standard sequence-to-sequence attentional model in two orthogonal ways
    <ul>
      <li>copy words from the source text via pointing</li>
      <li>use coverage to keep track of what has been summarized</li>
    </ul>
  </li>
  <li>Although our best model is abstractive, it <strong>does not produce novel n-grams</strong> (i.e., n-grams that don’t appear in the source text) as often as the reference summaries. The baseline model produces more novel n-grams, but many of these are erroneous
<img src="http://localhost:5000/images/tgfif/pointer.png" alt="summarization model" /></li>
</ul>

<hr />

<h3 id="sigir-2019-divgraphpointer-a-graph-pointer-network-for-extracting-diverse-keyphrases">[SIGIR 2019] <a href="https://edward-sun.github.io/files/DivGraphPointer.pdf">DivGraphPointer: A Graph Pointer Network for Extracting Diverse Keyphrases</a></h3>

<ul>
  <li>presents an end-to-end method called DivGraphPointer for extracting a set of diversified keyphrases from a document</li>
  <li>given a document, a word graph is constructed from the document based on <strong>word proximity</strong> and is encoded with <strong>graph convolutional networks</strong></li>
  <li>effectively capture document-level word salience by modeling <strong>long-range dependency</strong> between words in the document and <strong>aggregating multiple appearances of identical words</strong> into one node</li>
  <li>propose a diversified point network to generate a set of diverse keyphrases <strong>out of the word graph</strong> in the decoding process</li>
</ul>

<p><img src="http://localhost:5000/images/tgfif/graphpointermodel.png" alt="graphpointer model" />
<img src="http://localhost:5000/images/tgfif/graphpointer.png" alt="graphpointer results" /></p>

<hr />

<h3 id="arxiv19-the-curious-case-of-neural-text-degeneration">[arxiv19] <a href="https://arxiv.org/pdf/1904.09751.pdf">The Curious Case of Neural Text Degeneration</a></h3>

<p><img align="center" src="http://localhost:5000/images/tgfif/beamsearch.png" alt="scores" width="350" />
<img align="center" src="http://localhost:5000/images/tgfif/beamsearch2.png" alt="scores" width="350" /></p>

<h3 id="arxiv19-sensebert-driving-some-sense-into-bert">[arxiv19] <a href="https://arxiv.org/pdf/1908.05646.pdf">SenseBERT: Driving Some Sense into BERT</a></h3>
<ul>
  <li>propose a method to employ selfsupervision directly at the word sense level</li>
  <li>SenseBERT is pre-trained to predict not only the masked words but also their WordNet supersenses –&gt;  a lexical-semantic level language model</li>
</ul>

<p><img src="http://localhost:5000/images/tgfif/sensebert.png" alt="sensebert" /></p>
:ET
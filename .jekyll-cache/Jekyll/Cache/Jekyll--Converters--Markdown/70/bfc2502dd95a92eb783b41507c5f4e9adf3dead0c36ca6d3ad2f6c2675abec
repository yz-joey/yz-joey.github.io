I"E<h3 id="cvpr-2018-bottom-up-and-top-down-attention-for-image-captioning-and-visual-question-answering">[CVPR 2018] <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.pdf">Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering</a></h3>

<ul>
  <li><strong>image processing</strong>: the bottom-up mechanism (based on Faster R-CNN) proposes image regions, each with an associated feature vector, while the top-down mechanism determines feature weightings</li>
  <li>use Faster R-CNN (an object detection model designed to identify instances of objects belonging to certain classes and localize them with bounding boxes) in conjunction with the ResNet-101 CNN</li>
  <li><strong>captioning model</strong> uses a ‘soft’ top-down attention mechanism to weight each feature during caption generation, using the existing partial output sequence as context</li>
</ul>

<hr />
<p><img src="http://localhost:5000/images/tgfif/model1.png" alt="captioning model" /></p>

<hr />

<ul>
  <li>results on the MSCOCO test server establish a new state-of-the-art for the task, achieving CIDEr / SPICE / BLEU-4 scores of 117.9, 21.5 and 36.9</li>
</ul>

<hr />
<!--![evaluation metric](./images/tgfif/eval1.png)-->
<p><img src="http://localhost:5000/images/tgfif/eval1.png" alt="drawing" width="500" /></p>

<hr />

<h3 id="eccv-2018-advise-symbolism-and-external-knowledge-for-decoding-advertisements">[ECCV 2018] <a href="http://people.cs.pitt.edu/~kovashka/ye_kovashka_advise_eccv2018.pdf">ADVISE: Symbolism and External Knowledge for Decoding Advertisements</a></h3>

<ul>
  <li>question answering is formulated as a classification task: For each image, we use three related statements (i.e. statements provided by humans for this image) and randomly sample 47 unrelated statements</li>
  <li>the distance between an image and its corresponding statement should be smaller than the distance between that image and any other statement</li>
  <li>extract the image’s <a href="https://arxiv.org/pdf/1602.07261.pdf">Inception-v4</a> CNN feature (1536-D), then use a fully-connected layer with parameter w ∈ R^(200×1536) to project it to the 200-D joint embedding space</li>
  <li>use mean-pooling (<strong>a. comparable performance to the LSTM; b. better interpretability</strong>) to aggregate word embedding vectors into 200-D text embedding t and use GloVe to initialize the embedding matrix</li>
  <li>use the <a href="http://openaccess.thecvf.com/content_cvpr_2017/papers/Hussain_Automatic_Understanding_of_CVPR_2017_paper.pdf">SSD object detection model</a>, pretrain it on the <a href="http://cocodataset.org/#home">COCO</a> dataset, and fine-tune it with the symbol bounding box annotations</li>
  <li>use <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.pdf">bottom-up attention</a>(previous paper) to aggregate the information from symbolic regions –&gt; <strong>intuition</strong> is that ads draw the viewer’s attention in a particular way, and the symbol bounding boxes, without symbol labels, can be used to approximate this</li>
</ul>

<hr />
<p><img src="http://localhost:5000/images/tgfif/model2.png" alt="captioning model2" /></p>

<hr />

<ul>
  <li>use the DenseCap model to generate image captions and treat these as pre-fetched knowledge</li>
  <li>use a predicted symbol distribution at both training and test time
as a secondary image representation</li>
  <li>use a train/val/test split of 60%/20%/20%, resulting in around 39,000 images and more than 111,000 associated statements for training</li>
  <li>compute two metrics: Rank and Recall@3</li>
</ul>

<p><strong>Drawbacks:</strong></p>

<ul>
  <li>big ratio of visual ads are straightforward and do not contain symbolic features or rehtoric designs. In these cases, constraints via symbols could not be applied</li>
  <li>the weights for symbol-based and object-based constraints are manually set as 0.1</li>
  <li>external symbolic knowledge base is shallow: the symbol size is limited; only 5 related words are related to each symbol instead of a word distribution</li>
  <li>compared with generation, classification task has a finite searching space, which is much simpler</li>
</ul>

<h3 id="aaai19-kvqa-knowledge-aware-visual-question-answering">[AAAI19] <a href="http://dosa.cds.iisc.ac.in/kvqa/KVQA-AAAI2019.pdf">KVQA: Knowledge-aware Visual Question Answering</a></h3>
<ul>
  <li>In conventional VQA, one may ask questions about an image which can be answered <strong>purely based on its content</strong>; More recently, there is growing interest in answering questions which require <strong>commonsense knowledge</strong> involving common nouns (e.g., cats, dogs, microphones);  the important problem of answering questions requiring <strong>world knowledge</strong> about named entities.</li>
  <li>KVQA con- sists of 183K question-answer pairs involving more than 18K named entities and 24K images. Questions in this dataset require <strong>multi-entity, multi-relation, and multi-hop reasoning</strong> over large Knowledge Graphs (KG) to arrive at an answer.</li>
  <li><strong>Obviously, the knowledge graph can be Wikidata (Vrandecic and Kro ̈tzsch 2014).</strong></li>
</ul>

<hr />
<p><img src="http://localhost:5000/images/tgfif/VQAs.png" alt="VQA datasets" /></p>

<hr />

<ul>
  <li>We used the latest RDF dump (dated: 05-05-2018) of this KG. It stores facts in the form of triples, i.e., subject, relation and object.</li>
  <li><a href="http://malllabiisc.github.io/resources/kvqa/">Project website</a> containing the dataset and codes</li>
  <li>We choose memory network (memNet) (Weston, Chopra, and Bordes 2014) as one of our baselines for KVQA. Memory network provides a general architecture for learning from external knowledge.</li>
  <li>Each knowledge and spatial fact is fed to BLSTM to get corresponding memory embeddings <em>mi</em>. Question embeddings (q) for a question (Q) is also obtained in a similar fashion.</li>
</ul>

<hr />
<p><img src="http://localhost:5000/images/tgfif/kb-vqa.png" alt="KB VQA model" /></p>

<hr />
<h3 id="cvpr-2019-ok-vqa-a-visual-question-answering-benchmark-requiring-external-knowledge">[CVPR 2019] <a href="http://openaccess.thecvf.com/content_CVPR_2019/papers/Marino_OK-VQA_A_Visual_Question_Answering_Benchmark_Requiring_External_Knowledge_CVPR_2019_paper.pdf">OK-VQA: A Visual Question Answering Benchmark Requiring External Knowledge</a></h3>

<ul>
  <li>In this paper, we address the task of knowledge-based visual question answering and provide a benchmark, called OK-VQA, where the image content <strong>is not sufficient to answer the questions</strong>, encouraging methods that rely on external knowledge resources.</li>
  <li><strong>ArticleNet (AN)</strong>: We consider a simple knowledge-based baseline that we refer to as ArticleNet. The idea is to retrieve some articles from Wikipedia for each question-image pair and then train a network to find the answer in the retrieved articles.</li>
  <li>Retrieving articles is composed of three steps
    <ul>
      <li>come up with possible queries for each question</li>
      <li>use the Wikipedia search API to get the top retrieved article for each query</li>
      <li>extract a small subset (sentences) of each article that is most relevant for the query</li>
    </ul>
  </li>
  <li>pick the top scoring word [highest value of a(wi)*a(sent)] among the retrieved sentences to find the answer to a question</li>
  <li>ArticleNet is particularly helpful for <strong>brands</strong>, science, and cooking categories, perhaps suggesting that these categories are better represented in Wikipedia</li>
</ul>

<hr />
<p><img src="http://localhost:5000/images/tgfif/kb-vqa2.png" alt="KB VQA model2" /></p>

<hr />
<ul>
  <li><a href="https://arxiv.org/pdf/1705.06676.pdf">MUTAN</a>: Multimodal Tucker Fusion (MUTAN) model, a recent state-of-the-art tensor-based method for VQA</li>
</ul>

<p><img src="http://localhost:5000/images/tgfif/mutan.png" alt="MUTAN" /></p>

<hr />
<ul>
  <li>dataset is indeed visually grounded, but better image features do not hugely improve the results –&gt; difficulty lies in the retrieving the relevant knowledge and reasoning required to answer the questions</li>
</ul>

<h3 id="cvpr-2016-densecap-fully-convolutional-localization-networks-for-dense-captioning">[CVPR 2016] <a href="https://cs.stanford.edu/people/karpathy/densecap.pdf">DenseCap: Fully Convolutional Localization Networks for Dense Captioning</a></h3>
<ul>
  <li>introduce the dense captioning task, which requires a computer vision system to both <strong>localize and describe salient regions</strong> in images in natural language</li>
  <li>propose a <strong>Fully Convolutional Localization Network (FCLN)</strong> architecture that
processes an image with a single, efficient forward pass, requires <strong>no external regions proposals</strong>, and can be trained end-to-end with <strong>a single round of optimization</strong> (composed of a Convolutional Network, <strong>a novel dense localization layer</strong>, and Recurrent Neural Network language model that generates the label sequences)</li>
</ul>

<hr />
<p><img src="http://localhost:5000/images/tgfif/decap.png" alt="KB VQA model2" /></p>

<hr />
<ul>
  <li>use the VGG-16 architecture for its state-of-the-art performance</li>
  <li>localization layer is based on that of <strong>Faster R-CNN</strong> but replace their <strong>RoI pooling mechanism</strong> with <strong>bilinear interpolation</strong></li>
  <li>RoI pooling layer: gradients can be propagated backward from the output features to the input features, but not to the input proposal coordinates –&gt; <strong>bilinear interpolation</strong></li>
</ul>

<h3 id="iccv-2015-faster-r-cnn">[ICCV 2015] <a href="https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Girshick_Fast_R-CNN_ICCV_2015_paper.pdf">Faster R-CNN</a></h3>
<ul>
  <li>complexity arises because <strong>detection requires the accurate localization of objects</strong>:
    <ul>
      <li>numerous candidate object locations (often called “proposals”) must be processed</li>
      <li>these candidates provide only rough localization that must be refined</li>
    </ul>
  </li>
  <li>propose a <strong>single-stage</strong> training algorithm that <strong>jointly learns</strong> to classify object proposals and refine their spatial locations (jointly optimizes a softmax classifier and bounding-box regressors)</li>
  <li>fast R-CNN has higher detection quality (mAP) than R-CNN, SPPnet; training is single-stage, using a multi-task loss ; training can update all network layers; no disk storage is required for feature caching</li>
</ul>

<hr />
<!--![evaluation metric](./images/tgfif/eval1.png)-->
<p><img src="http://localhost:5000/images/tgfif/rcnn.png" alt="drawing" width="500" /></p>

<hr />
<ul>
  <li>fast R-CNN network takes as input <strong>an entire image and a set of object proposals</strong>
    <ul>
      <li>processes the whole image with several convolutional (conv) and max pooling layers to produce <strong>a conv feature map</strong></li>
      <li>for each object proposal <strong>a region of interest (RoI)</strong> pooling layer extracts <strong>a fixed-length feature vector</strong> from the feature map</li>
    </ul>
  </li>
  <li>SGD minibatches are sampled <strong>hierarchically</strong>, first by sampling N images and then by sampling R/N RoIs from each image</li>
  <li>large fully connected layers are easily accelerated by compressing them with truncated SVD</li>
</ul>

<h3 id="cvpr-2018-learning-answer-embeddings-for-visual-question-answering">[CVPR 2018] <a href="http://openaccess.thecvf.com/content_cvpr_2018/papers/Hu_Learning_Answer_Embeddings_CVPR_2018_paper.pdf">Learning Answer Embeddings for Visual Question Answering</a></h3>
<ul>
  <li>the key idea is to infer two sets of embeddings: one for the image and the question jointly and the other for the answers</li>
  <li>take the semantic relationships (as characterized by the embeddings) among answers into consideration</li>
  <li>the answer embeddings parameterize a probabilistic model describing how the answers are similar to the image and question pair</li>
  <li>joint embedding function <strong>fθ (i, q)</strong> to generate the embedding of the pair i and q; an embedding function <strong>gφ(a)</strong> to generate the embedding of an answer a; decision rule relies on computing <strong>fθ(i, q)⊤gφ(a)</strong>, a factorized form of the more generic function <strong>h(i, q, a)</strong></li>
  <li>use two different models to parameterize the embedding function <strong>fθ(i,q)</strong> in our experiments
    <ul>
      <li><strong>Multi-layer Perceptron (MLP)</strong> and <strong>Stacked Attention Network (SAN)</strong></li>
      <li>represent each token in the question by 300-d <strong>GloVe</strong> vector, and use the <strong>ResNet-152</strong> to extract the visual features</li>
    </ul>
  </li>
  <li>For parameterizing the answering embedding function <strong>gφ(a)</strong>, we adopt two architectures:
    <ul>
      <li>utilizing a <strong>one-layer MLP</strong> on average GloVe embeddings of answer sequences, with the output dimensionality of 1,024</li>
      <li>utilizing a <strong>two-layer bidirectional LSTM (bi-LSTM)</strong> on top of GloVe embeddings of answer sequences</li>
    </ul>
  </li>
</ul>

<h3 id="cvpr-2016-learning-deep-structure-preserving-image-text-embeddings">[CVPR 2016] <a href="https://arxiv.org/pdf/1511.06078.pdf">Learning Deep Structure-Preserving Image-Text Embeddings</a></h3>
<ul>
  <li>learning joint embeddings of images and text using a two-branch neural network with multiple layers of linear projections followed by nonlinearities</li>
  <li>network is trained using <strong>a large margin objective</strong> that combines <strong>cross-view ranking constraints</strong> with <strong>within-view neighborhood structure preservation constraints</strong> inspired by metric learning literature</li>
  <li>several recent embedding methods are based on <strong>Canonical Correlation Analysis</strong> (CCA), which finds linear projections that maximize the correlation between projected vectors from the two views –&gt; hard to scale wit large amunts of data</li>
  <li>alternative to CCA is to learn a joint embedding space using SGD with a ranking loss</li>
  <li>in the learned latent space, we want images (resp. sentences) with similar meaning to be close to each other
    <ul>
      <li>the <strong>Large Margin Nearest Neighbor (LMNN)</strong> approach tries to ensure that for each image its target neighbors <strong>from the same class</strong> are closer than samples from other classes</li>
    </ul>
  </li>
</ul>

<hr />
<!--![evaluation metric](./images/tgfif/eval1.png)-->
<p><img src="http://localhost:5000/images/tgfif/image-text1.png" alt="drawing" width="300" /></p>

<hr />
<ul>
  <li><strong>Bi-directional ranking constraints</strong>: the distance between xi and each positive sentence yj to be smaller than the distance between xi and each negative sentence yk by some enforced margin m</li>
</ul>

<p><img src="http://localhost:5000/images/tgfif/ranking-constraint.png" alt="drawing" width="500" /></p>

<ul>
  <li><strong>Structure-preserving constraints</strong> enforce a margin of m between N(xi) and any point outside of the neighborhood</li>
</ul>

<p><img src="http://localhost:5000/images/tgfif/structure-preserving.png" alt="drawing" width="550" /></p>

<ul>
  <li><strong>Embedding Loss Function</strong> hinge loss</li>
</ul>

<p><img src="http://localhost:5000/images/tgfif/loss.png" alt="drawing" width="500" /></p>

<ul>
  <li>report results on <strong>image-tosentence and sentence-to-image retrieval</strong> on the standard Flickr30K and MSCOCO datasets. Flickr30K consists of 31783 images accompanied by five descriptive sentences each. The larger MSCOCO dataset consists of 123000 images, also with five sentences each</li>
</ul>

<h3 id="arxiv-2019-vilbert-pretraining-task-agnostic-visiolinguistic-representations-for-vision-and-language-tasks">[Arxiv 2019] <a href="https://arxiv.org/pdf/1908.02265.pdf">ViLBERT: Pretraining Task-Agnostic Visiolinguistic Representations for Vision-and-Language Tasks</a></h3>

<ul>
  <li><img src="http://localhost:5000/images/tgfif/vilbert0.png" alt="vilbert0" width="70" /></li>
  <li>ViLBERT (short for Vision-and-Language BERT), a model for learning
task-agnostic <strong>joint representations of image content and natural language</strong>.</li>
  <li>
    <p>extend the popular BERT architecture to a <strong>multi-modal two-stream model</strong>, processing both visual and textual inputs in separate streams that interact through co-attentional transformer layers</p>
  </li>
  <li>overall archtitecture</li>
</ul>

<p><img src="http://localhost:5000/images/tgfif/vilbert1.png" alt="vilbert1" width="700" /></p>

<ul>
  <li>co-attention transformer</li>
</ul>

<p><img src="http://localhost:5000/images/tgfif/vilbert2.png" alt="vilbert2" width="700" /></p>

<ul>
  <li>pretraining tasks for loss</li>
</ul>

<p><img src="http://localhost:5000/images/tgfif/vilbert3.png" alt="vilbert3" width="700" /></p>

<ul>
  <li>Downstream tasks</li>
</ul>

<p><img src="http://localhost:5000/images/tgfif/vilbert4.png" alt="vilbert4" width="700" /></p>

<!--
## few shot learning 
## generative adversarial network
### Text-Adaptive Generative Adversarial Networks: Manipulating Images with Natural Language (Seonghyeon Nam)

## variational autoencoder
-->
:ET
I"Ì;<h3 id="openaiblog19-language-models-are-unsupervised-multitask-learners-gpt-2-porject">[OpenAIBlog19] <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf">Language Models are Unsupervised Multitask Learners (GPT-2)</a> <a href="https://openai.com/blog/better-language-models/">[porject]</a></h3>

<ul>
  <li>GPT-2 is a large <strong>transformer-based</strong> language model, which is trained with a simple objective: <strong>predict the next word</strong>, given all of the previous words within some text</li>
  <li>GPT-2 is a direct <strong>scale-up of GPT</strong>, with more than <strong>10X the parameters</strong> and trained on more than <strong>10X the amount of data</strong></li>
  <li>We demonstrate language models can perform down-stream tasks in a zero-shot setting ‚Äì without any parameter or architecture modification</li>
  <li>It should model p(output|input, task), which has been variously formalized in multitask and meta-learning settings</li>
  <li>Get surprising results without any fine-tuning of GPT-2 models on some language tasks like question answering, reading comprehension, summarization, and translation</li>
</ul>

<h3 id="naacl19-bert-has-a-mouth-and-it-must-speak-bert-as-a-markov-random-field-language-model-demo">[NAACL19] <a href="https://arxiv.org/pdf/1902.04094.pdf">BERT has a Mouth, and It Must Speak: BERT as a Markov Random Field Language Model</a> <a href="https://colab.research.google.com/drive/1MxKZGtQ9SSBjTK5ArsZ5LKhkztzg52RV#scrollTo=9TPa6BsRTulK">[demo]</a></h3>
<ul>
  <li>We show that BERT (Devlin et al., 2018) is a <strong>Markov random field language model</strong>.</li>
  <li>Compared to the generations of a traditional left-to-right language model, BERT generates sentences that are <strong>more diverse</strong> but <strong>of slightly worse quality</strong></li>
  <li>Because the model expects context from <strong>both directions</strong>, it is not immediately obvious how BERT can be used as a traditional language model</li>
  <li>BERT is a combination of a <strong>Markov random field language model</strong> (MRF-LM, Jernite et al., 2015; Mikolov et al., 2013) with <strong>pseudo loglikelihood (Besag, 1977) training</strong></li>
  <li>We can use BERT to compute the <strong>unnormalized log-probabilities log œÜt(X)</strong> to find the most likely sentence within the set.</li>
  <li>We experiment with generating from BERT in a <strong>left-to-right</strong> manner.</li>
  <li>Three functions:
    <ul>
      <li>parallel_sequential_generation:Generate for one random position at a timestep</li>
      <li>parallel_generation: Generate for all positions at each time step</li>
      <li>sequential_generation: Generate one word at a time, in L-&gt;R order</li>
    </ul>
  </li>
</ul>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def sequential_generation(seed_text, batch_size=10, max_len=15, leed_out_len=15, 
                          top_k=0, temperature=None, sample=True, cuda=False):
    """ Generate one word at a time, in L-&gt;R order """
    seed_len = len(seed_text) 
    batch = get_init_text(seed_text, max_len, batch_size) 
    # [[CLS], [MASK], [MASK], ..., [MASK], [SEP]] * batch_size
    
    for ii in range(max_len):
        inp = [sent[:seed_len+ii+leed_out_len]+[sep_id] for sent in batch]
        inp = torch.tensor(batch).cuda() if cuda else torch.tensor(batch)
        out = model(inp)
        idxs = generate_step(out, gen_idx=seed_len+ii, top_k=top_k, temperature=temperature, sample=sample)
        # find the best word from top-k choice
        for jj in range(batch_size):
            batch[jj][seed_len+ii] = idxs[jj]
        
    return untokenize_batch(batch)
</code></pre></div></div>
<p><strong>comment:</strong></p>

<ul>
  <li>This paper mathematically explained why BERT can be used as a generator</li>
  <li>But the generation method is obvious and trivial</li>
  <li>The ‚Äúwild‚Äù generation is in an unsupervised manner and could not serve the machine translation task or text summarization task</li>
</ul>

<h3 id="arxiv1904-pretraining-based-natural-language-generation-for-text-summarization">[Arxiv1904] <a href="https://arxiv.org/pdf/1902.09243.pdf">Pretraining-Based Natural Language Generation for Text Summarization</a></h3>

<ul>
  <li>We encode the input sequence into context representations using BERT</li>
  <li>For the decoder, there are two stages in our model:
    <ul>
      <li>use a Transformer-based decoder to generate a draft output sequence</li>
      <li>three steps:
        <ul>
          <li>mask each word of the draft sequence and feed it to BERT</li>
          <li>combining the input sequence and the draft representation generated by BERT</li>
          <li>use a Transformer-based decoder to predict the refined word for each masked position</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>The decoder <strong>cannot</strong> utilize BERT‚Äôs ability to generate high quality context vectors, which will also <strong>harm performance</strong>, but can use BERT‚Äôs contextualized representations to enhance the decoder in the <strong>refine process</strong></li>
  <li>Introduce <strong>BERT‚Äôs word embedding matrix</strong> to map the previous summary draft outputs {y1, . . . , yt‚àí1} into embeddings vectors {q1, . . . , qt‚àí1} at t_th time step</li>
  <li>Incorporate <strong>copy mechanism</strong> <a href="https://arxiv.org/pdf/1603.06393.pdf">[Gu et al., 2016] COPYNET</a> based on the Transformer decoder</li>
  <li><strong>BERT</strong>_LARGE needs unacceptable memory usage</li>
  <li>We <strong>filter repeated tri-grams in beam-search process</strong> by setting word probability to zero if it will generate an tri-gram which exists in the existing summary. It is a nice method to avoid phrase repetition since the two datasets seldom contains repeated tri-grams in one summary</li>
</ul>

<p><img align="center" src="http://localhost:5000/images/lmg/bertgen1.png" alt="bertgen1" width="700" />
<img align="center" src="http://localhost:5000/images/lmg/bertgen2.png" alt="bertgen2" width="700" /></p>

<p><strong>Comments:</strong></p>

<ul>
  <li>Copy mechanism is same as Pointer-Generater but replace the RNN in seq2seq by transformer; not sure if this is the first work of doing so</li>
  <li>The imporvement on CNN and Daily Mail Dataset is not significant at all. It is not strict without a variance described</li>
  <li>The revise stage seems meaningless compared to the next work (see the BERT in result table);</li>
  <li>However, this work is a great trial of using deep language models (BERT) to generate text considering BERT is an auto-encoder based LM</li>
  <li><strong>Here rasied a question: will ELMo, GPT-2, and XLNet better choices to generate text because of their auto-regressive property?</strong></li>
  <li>A Chinese <a href="https://zhuanlan.zhihu.com/p/59757422">Blog</a></li>
</ul>

<h3 id="acl19-hibert-document-level-pre-training-of-hierarchical-bidirectional-transformers-for-document-summarization">[ACL19] <a href="https://www.aclweb.org/anthology/P19-1499">HIBERT: Document Level Pre-training of Hierarchical Bidirectional Transformers for Document Summarization</a></h3>
<ul>
  <li>Neural extractive summarization models usually employ a <strong>hierarchical encoder</strong> for document encoding and they are trained using sentence-level labels, which are created heuristically using rule-based methods and inaccurate</li>
  <li>Both the sentence encoder and document encoder are based on the Transformer encoder</li>
  <li>Document Masking: in summarization, context on both directions are available, so we
opt to predict a sentence using all sentences on both its left and right</li>
  <li>Our model is trained in three stages, which includes two pre-training stages and one finetuning stage:
    <ul>
      <li>open-domain pretraining and in this stage we train HIBERT on GIGA-CM dataset</li>
      <li>perform the indomain pre-training on the CNNDM (or NYT50)</li>
      <li>finetune HIBERT in the summarization model to predict extractive sentence labels</li>
    </ul>
  </li>
</ul>

<p><img align="center" src="http://localhost:5000/images/lmg/hibert1.png" alt="hibert1" width="300" />
<img align="center" src="http://localhost:5000/images/lmg/hibert2.png" alt="hibert2" width="300" /></p>

<p><strong>Comments:</strong></p>

<ul>
  <li>This work explores how to modify the architecture of BERT and pretrain it and finetune it</li>
  <li>Hirarchical BERT is good for document summarization and keyphrase generation, but will not benefit (visual) question answering task</li>
  <li>This work did not see the text generation loss for downstream task (summarization) and felt confused about how this model could generate text instead of tag of each sentence</li>
</ul>

<h3 id="icml19-mass-masked-sequence-to-sequence-pre-training-for-language-generation">[ICML19] <a href="https://arxiv.org/pdf/1905.02450.pdf">MASS: Masked Sequence to Sequence Pre-training for Language Generation</a></h3>

<p><img align="center" src="http://localhost:5000/images/lmg/mass1.png" alt="mass1" width="700" />
<img align="center" src="http://localhost:5000/images/lmg/mass2.png" alt="mass2" width="700" /></p>

<ul>
  <li>MASS adopts the <strong>encoder-decoder framework</strong> to reconstruct a sentence fragment given the remaining part of the sentence:
    <ul>
      <li>its encoder takes a sentence with randomly masked fragment (several consecutive tokens) as input</li>
      <li>its decoder tries to predict this masked fragment</li>
    </ul>
  </li>
  <li>MASS is carefully designed to <strong>pre-train both the encoder and decoder jointly</strong> using only unlabeled data, and can be applied to most language generations tasks</li>
  <li>We have an important hyperparameter k, which denotes the length of the masked fragment of the sentence. Our method with different k values can cover the special cases that are related to previous pre-training methods</li>
  <li>While achieving promising results on language understanding tasks, BERT <strong>is not suitable</strong> for language generation tasks which typically leverage an encoder-decoder framework for conditional sequence generation</li>
  <li>By by predicting <strong>consecutive tokens</strong> in the decoder side, the decoder can build better language modeling capability than just predicting discrete tokens</li>
  <li>MASS achieves best performance on these downstream tasks when k is nearly <strong>50% of the sentence length</strong> m</li>
</ul>

<h3 id="emnlp19-text-summarization-with-pretrained-encoders-code">[EMNLP19] <a href="https://arxiv.org/pdf/1908.08345.pdf">Text Summarization with Pretrained Encoders</a> <a href="https://github.com/nlpyang/PreSumm">[code]</a></h3>

<p><img align="center" src="http://localhost:5000/images/lmg/bertsum1.png" alt="bertsum1" width="700" /></p>

<ul>
  <li>We use <strong>a standard encoder-decoder framework for abstractive summarization</strong> (See et al., 2017). The encoder is the pretrained BERTSUM and the decoder is a 6-layered Transformer initialized randomly</li>
  <li>There is a <strong>mismatch between the encoder and the decoder</strong>, since the former is pretrained while the latter must be trained from scratch ‚Äì&gt; <strong>make fine-tuning unstable</strong> (the encoder might overfit the data while the decoder underfits) ‚Äì&gt; we design a new fine-tuning schedule which <strong>separates the optimizers of the encoder and the decoder</strong></li>
  <li>Using extractive objectives can boost the performance of abstractive summarization</li>
  <li>Our decoder applies neither a copy nor a coverage mechanism (See et al., 2017), because we focus on building a minimum-requirements model and these mechanisms may introduce additional hyper-parameters to tune</li>
</ul>

<p><img align="center" src="http://localhost:5000/images/lmg/bertsum2.png" alt="bertsum2" width="350" /></p>

<p><strong>Comments:</strong></p>

<ul>
  <li>This work is indeed a minimum-requirements model with an original bert structure and adaptive data format</li>
  <li>The best results of CNN/DailyNews so far were mentioned</li>
  <li>The trick of separate optimizers is interesting but hyperparameter tuning metric is not solid</li>
  <li>The codes are written in pytorch, which is distributed and convenient to extend</li>
</ul>

<h3 id="emnlp19-denoising-based-sequence-to-sequence-pre-training-for-text-generation-code">[EMNLP19] <a href="https://arxiv.org/pdf/1908.08206.pdf">Denoising based Sequence-to-Sequence Pre-training for Text Generation</a> <a href="https://github.com/yuantiku/PoDA">[code]</a></h3>

<ul>
  <li>Unlike encoder-only (e.g., BERT) or decoder-only (e.g., OpenAI GPT) pre-training approaches, PoDA jointly <strong>pretrains both the encoder and decoder</strong> by denoising the noise-corrupted text</li>
  <li>It keeps the network architecture unchanged in the subsequent fine-tuning stage</li>
  <li>We design a <strong>hybrid model of Transformer and pointer-generator networks</strong> as the backbone architecture for PoDA</li>
  <li>Like denoising autoencoders, PoDA works by denoising the noise-corrupted text sequences. There are three types of noises: <strong>randomly shuffle, delete or replace the words</strong> in a given sequence.</li>
</ul>

<p><img src="http://localhost:5000/images/lmg/poda1.png" alt="poda1" width="700" /></p>

<p><strong>Comments:</strong></p>

<ul>
  <li>This work is interesting because it leveraged denosing autoencoder to pretrain the langage model</li>
  <li>Both BERT and GPT‚Äôs loss functions are used, which tackles the problem that BERT as an autoencoder language model, is not designed for text generation</li>
  <li>The analysis of linguistic quality is not solid with several sets of case study</li>
  <li>The difference between BERTSUM and PoDA:
    <ul>
      <li>BERTSUM only pretrains encoder and uses different optimizers to tackle the mismatch between decoder and encoder but PoDA pretrains both encoder and decoder</li>
      <li>The training objective of BERTSUM is a document-version of BERT but PoDA uses autoencoder denosing method</li>
      <li>After comparing these two models, I prefer PoDA because it works as a whole and BERTSUM did a trivial improvement model-wise</li>
    </ul>
  </li>
</ul>

<h3 id="nips17-attention-is-all-you-need-tensorflow-pytorch">[NIPS17] <a href="https://arxiv.org/pdf/1706.03762.pdf">Attention Is All You Need</a> <a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch">[tensorflow]</a> <a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch">[pytorch]</a></h3>

<p><strong>Comments:</strong></p>

<ul>
  <li><a href="http://jalammar.github.io/illustrated-transformer/">Here</a> illustrated the mechanism of transformer decoder generating text:‚Ä®<img align="center" src="http://jalammar.github.io/images/t/transformer_decoding_1.gif" alt="bertgen1" width="700" />
<img align="center" src="http://jalammar.github.io/images/t/transformer_decoding_2.gif" alt="bertgen1" width="700" /></li>
</ul>

<h3 id="appendix-bert-related-papers">[Appendix] <a href="https://github.com/tomohideshibata/BERT-related-papers">BERT Related Papers</a></h3>
:ET